{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ecfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, GlobalMaxPooling1D, MaxPooling2D, Flatten, Dense, Input, Concatenate, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd8c58",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "\n",
    "2 tier CNN\n",
    "\n",
    "First tier predicts timezone\n",
    "\n",
    "Second tier predicts state inside indicated timezone from tier 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd8d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.read_csv('All_US_Time_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674a5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('west_time_tweets.csv')\n",
    "to_removeca = df1[df1['Closest_State'] == 'California'].sample(n = 10000, random_state = 40)\n",
    "df1 = df1.drop(to_removeca.index)\n",
    "\n",
    "\n",
    "df2 = pd.read_csv('central_time_tweets.csv')\n",
    "\n",
    "df3 = pd.read_csv('east_time_tweets.csv')\n",
    "to_removeny = df3[df3['Closest_State'] == 'New York'].sample(n = 40000, random_state = 40)\n",
    "to_removenj = df3[df3['Closest_State'] == 'New Jersey'].sample(n = 20000, random_state = 40)\n",
    "df3 = df3.drop(to_removeny.index)\n",
    "df3 = df3.drop(to_removenj.index)\n",
    "\n",
    "df4 = pd.read_csv('mountain_time_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d0a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa['TweetText'] = dfa['TweetText'].astype(str)\n",
    "df1['TweetText'] = df1['TweetText'].astype(str)\n",
    "df2['TweetText'] = df2['TweetText'].astype(str)\n",
    "df3['TweetText'] = df3['TweetText'].astype(str)\n",
    "df4['TweetText'] = df4['TweetText'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4c620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dfa['TweetText'].tolist())\n",
    "\n",
    "to_removeny = dfa[dfa['Closest_State'] == 'New York'].sample(n = 40000, random_state = 40)\n",
    "to_removenj = dfa[dfa['Closest_State'] == 'New Jersey'].sample(n = 20000, random_state = 40)\n",
    "to_removeca = dfa[dfa['Closest_State'] == 'California'].sample(n = 10000, random_state = 40)\n",
    "dfa = dfa.drop(to_removeny.index)\n",
    "dfa = dfa.drop(to_removenj.index)\n",
    "dfa = dfa.drop(to_removeca.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a093dd3c-149a-4a34-8f6b-f61a47b913bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(dfa['Timezone'])\n",
    "dfa = dfa.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da4fa04-8a94-4608-9374-2b234b77e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_onehot(df):\n",
    "    one_hot = pd.get_dummies(df['Closest_State'])\n",
    "    df = df.join(one_hot)\n",
    "    #df = df.drop('Closest_State', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f069d1-4836-430f-9afd-e76b35035a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = make_onehot(df1)\n",
    "df2 = make_onehot(df2)\n",
    "df3 = make_onehot(df3)\n",
    "df4 = make_onehot(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "410d2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = dfa[['TweetText','Timestamp']]\n",
    "y_data = dfa[dfa['Timezone'].unique()]\n",
    "\n",
    "x_dat1 = df1[['TweetText','Timestamp']]\n",
    "y_dat1 = df1[df1['Closest_State'].unique()]\n",
    "\n",
    "x_dat2 = df2[['TweetText','Timestamp']]\n",
    "y_dat2 = df2[df2['Closest_State'].unique()]\n",
    "\n",
    "x_dat3 = df3[['TweetText','Timestamp']]\n",
    "y_dat3 = df3[df3['Closest_State'].unique()]\n",
    "\n",
    "x_dat4 = df4[['TweetText','Timestamp']]\n",
    "y_dat4 = df4[df4['Closest_State'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68310a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traina, x_testa, y_traina, y_testa = train_test_split(x_data, y_data, test_size = 0.25, random_state = 40)\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x_dat1, y_dat1, test_size = 0.25, random_state = 40)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_dat2, y_dat2, test_size = 0.25, random_state = 40)\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(x_dat3, y_dat3, test_size = 0.25, random_state = 40)\n",
    "x_train4, x_test4, y_train4, y_test4 = train_test_split(x_dat4, y_dat4, test_size = 0.25, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39d87a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    304519.000000\n",
       "mean         11.894046\n",
       "std           7.584947\n",
       "min           1.000000\n",
       "1%            2.000000\n",
       "50%          11.000000\n",
       "99%          29.000000\n",
       "max         710.000000\n",
       "Name: TweetText, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[\"TweetText\"].str.split(\" \").str.len().describe(percentiles=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb7071a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_length = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f295881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq(x_train, x_test):\n",
    "    train_sequences = tokenizer.texts_to_sequences(x_train['TweetText'].tolist())\n",
    "    test_sequences = tokenizer.texts_to_sequences(x_test['TweetText'].tolist())\n",
    "    \n",
    "    train_seq = pad_sequences(train_sequences, maxlen = tmax_length, padding = 'post', truncating = 'post')\n",
    "    test_seq = pad_sequences(test_sequences, maxlen = tmax_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "    return train_seq, test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb58e434-9b4c-46c2-a6c9-7befe9a19ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqa, test_seqa = get_seq(x_traina, x_testa)\n",
    "train_seq1, test_seq1 = get_seq(x_train1, x_test1)\n",
    "train_seq2, test_seq2 = get_seq(x_train2, x_test2)\n",
    "train_seq3, test_seq3 = get_seq(x_train3, x_test3)\n",
    "train_seq4, test_seq4 = get_seq(x_train4, x_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc5c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.index_word) + 1\n",
    "embedding_dim = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb334c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_traina = x_traina['Timestamp']\n",
    "timestamps_testa = x_testa['Timestamp']\n",
    "\n",
    "timestamps_train1 = x_train1['Timestamp']\n",
    "timestamps_test1 = x_test1['Timestamp']\n",
    "\n",
    "timestamps_train2 = x_train2['Timestamp']\n",
    "timestamps_test2 = x_test2['Timestamp']\n",
    "\n",
    "timestamps_train3 = x_train3['Timestamp']\n",
    "timestamps_test3 = x_test3['Timestamp']\n",
    "\n",
    "timestamps_train4 = x_train4['Timestamp']\n",
    "timestamps_test4 = x_test4['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "406d47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = Input(shape = (tmax_length,), name = 'Input Sequence')\n",
    "input_time = Input(shape = (1,), name = \"Input Timestamp\")\n",
    "\n",
    "embed = Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = tmax_length)(input_seq)\n",
    "convol = Conv1D(filters = 128, kernel_size = 3, activation = 'relu')(embed)\n",
    "maxpool = GlobalAveragePooling1D()(convol)\n",
    "\n",
    "concat = Concatenate()([maxpool, input_time])\n",
    "dense1 = Dense(50, activation = 'relu')(concat)\n",
    "dense2 = Dense(25, activation = 'relu')(dense1)\n",
    "dense3 = Dense(15, activation = 'relu')(dense2)\n",
    "output = Dense(len(dfa['Timezone'].unique()), activation = 'sigmoid')(dense3)\n",
    "\n",
    "modela = Model(inputs = [input_seq, input_time], outputs = output)\n",
    "modela.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "output = Dense(len(df1['Closest_State'].unique()), activation = 'sigmoid')(dense3)\n",
    "\n",
    "model1 = Model(inputs = [input_seq, input_time], outputs = output)\n",
    "model1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "output = Dense(len(df2['Closest_State'].unique()), activation = 'sigmoid')(dense3)\n",
    "\n",
    "model2 = Model(inputs = [input_seq, input_time], outputs = output)\n",
    "model2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "output = Dense(len(df3['Closest_State'].unique()), activation = 'sigmoid')(dense3)\n",
    "\n",
    "model3 = Model(inputs = [input_seq, input_time], outputs = output)\n",
    "model3.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "output = Dense(len(df4['Closest_State'].unique()), activation = 'sigmoid')(dense3)\n",
    "\n",
    "model4 = Model(inputs = [input_seq, input_time], outputs = output)\n",
    "model4.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83fa183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1523/1523\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 163ms/step - accuracy: 0.5853 - loss: 193.0028 - val_accuracy: 0.7096 - val_loss: 0.9043\n",
      "Epoch 2/3\n",
      "\u001b[1m1523/1523\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 162ms/step - accuracy: 0.7092 - loss: 0.8873 - val_accuracy: 0.7096 - val_loss: 0.8606\n",
      "Epoch 3/3\n",
      "\u001b[1m1523/1523\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 161ms/step - accuracy: 0.7072 - loss: 0.8620 - val_accuracy: 0.7096 - val_loss: 0.8534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d3edec87a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modela.fit([train_seqa, timestamps_traina], y_traina, epochs = 3, batch_size = 150, validation_data = ([test_seqa, timestamps_testa], y_testa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "056d5e04-7528-434c-804c-0eedc2fd72b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 155ms/step - accuracy: 0.8095 - loss: 1.2734 - val_accuracy: 0.8138 - val_loss: 0.9921\n",
      "Epoch 2/3\n",
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 156ms/step - accuracy: 0.8184 - loss: 0.9282 - val_accuracy: 0.8138 - val_loss: 0.7991\n",
      "Epoch 3/3\n",
      "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 156ms/step - accuracy: 0.8059 - loss: 0.7860 - val_accuracy: 0.8138 - val_loss: 0.7189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d3ed69d1c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit([train_seq1, timestamps_train1], y_train1, epochs = 3, batch_size = 50, validation_data = ([test_seq1, timestamps_test1], y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f4139fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 156ms/step - accuracy: 0.3358 - loss: 2.6329 - val_accuracy: 0.3473 - val_loss: 2.3315\n",
      "Epoch 2/3\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 156ms/step - accuracy: 0.3502 - loss: 2.2711 - val_accuracy: 0.3473 - val_loss: 2.1717\n",
      "Epoch 3/3\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 156ms/step - accuracy: 0.3493 - loss: 2.1488 - val_accuracy: 0.3473 - val_loss: 2.1266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d3ed9436b0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([train_seq2, timestamps_train2], y_train2, epochs = 3, batch_size = 50, validation_data = ([test_seq2, timestamps_test2], y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91104fd1-9415-4c80-acca-6ae3ae8c43d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1079/1079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 159ms/step - accuracy: 0.1687 - loss: 2.9033 - val_accuracy: 0.1670 - val_loss: 2.6745\n",
      "Epoch 2/3\n",
      "\u001b[1m1079/1079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 159ms/step - accuracy: 0.1700 - loss: 2.6479 - val_accuracy: 0.1670 - val_loss: 2.6077\n",
      "Epoch 3/3\n",
      "\u001b[1m1079/1079\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 160ms/step - accuracy: 0.1703 - loss: 2.5962 - val_accuracy: 0.1670 - val_loss: 2.5925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d3eeb69340>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit([train_seq3, timestamps_train3], y_train3, epochs = 3, batch_size = 150, validation_data = ([test_seq3, timestamps_test3], y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb477730-26cd-4e55-9348-d26827004b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 157ms/step - accuracy: 0.6933 - loss: 1.9345 - val_accuracy: 0.6836 - val_loss: 1.8393\n",
      "Epoch 2/3\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.7041 - loss: 1.8176 - val_accuracy: 0.6836 - val_loss: 1.7377\n",
      "Epoch 3/3\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 156ms/step - accuracy: 0.7026 - loss: 1.7074 - val_accuracy: 0.6836 - val_loss: 1.6488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d3f6cf0110>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit([train_seq4, timestamps_train4], y_train4, epochs = 3, batch_size = 50, validation_data = ([test_seq4, timestamps_test4], y_test4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a0d0dd5-599d-4632-8dee-d7b4fed819c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50132203, 0.1314287 , 0.363014  , 0.8041143 ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = train_seqa[0:1]\n",
    "time = timestamps_traina[0:1]\n",
    "modela.predict([text, time])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
