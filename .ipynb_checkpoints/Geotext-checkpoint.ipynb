{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OB3HZ386Ma21"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aDcfzaBKyDJi"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "dataset_path = r'data\\full_text\\clean_tweets.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "data = data[['TweetText', 'Closest_State']]\n",
    "\n",
    "# Remove tweets with outside of thresholds)\n",
    "data = data[data['TweetText'].str.split().str.len().between(15, 200)]\n",
    "# Remove duplicate tweets\n",
    "data = data.drop_duplicates(subset=['TweetText'])\n",
    "# Normalize text\n",
    "data['TweetText'] = data['TweetText'].str.lower().str.strip()\n",
    "# Remove URLs\n",
    "data['TweetText'] = data['TweetText'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x, flags=re.MULTILINE))\n",
    "# Remove special characters, hashtags, mentions\n",
    "data['TweetText'] = data['TweetText'].str.replace(r'[@#]\\S+', '', regex=True)  # Remove hashtags and mentions\n",
    "data['TweetText'] = data['TweetText'].str.replace(r'[^A-Za-z0-9\\s]', '', regex=True)  # Remove non-alphanumeric characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for states with at least 5,000 tweets\n",
    "state_counts = data['Closest_State'].value_counts()\n",
    "eligible_states = state_counts[state_counts >= 1000].index\n",
    "data = data[data['Closest_State'].isin(eligible_states)]\n",
    "\n",
    "# Encode the 'Closest_State' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['state_id'] = label_encoder.fit_transform(data['Closest_State'])\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data.to_csv(r\"data\\processed_tweets_train.csv\", index=False)\n",
    "val_data.to_csv(r\"data\\processed_tweets_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State to state_id mapping: {'Arizona': np.int64(0), 'California': np.int64(1), 'Connecticut': np.int64(2), 'Florida': np.int64(3), 'Georgia': np.int64(4), 'Illinois': np.int64(5), 'Louisiana': np.int64(6), 'Maryland': np.int64(7), 'Massachusetts': np.int64(8), 'Michigan': np.int64(9), 'New Jersey': np.int64(10), 'New York': np.int64(11), 'North Carolina': np.int64(12), 'Ohio': np.int64(13), 'Pennsylvania': np.int64(14), 'South Carolina': np.int64(15), 'Tennessee': np.int64(16), 'Texas': np.int64(17), 'Virginia': np.int64(18)}\n"
     ]
    }
   ],
   "source": [
    "# Print the state_id mappings\n",
    "print(\"State to state_id mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged training data saved as 'train_with_pfas_data.csv'.\n",
      "Merged validation data saved as 'val_with_pfas_data.csv'.\n",
      "                                           TweetText Closest_State  state_id  \\\n",
      "0  there is a tutorial thingy that should walk u ...       Georgia         4   \n",
      "1  rt  so my cousin new gfs bad hmm time to steal...      New York        11   \n",
      "2   if u paid 150 on ur hair and soon as u leave ...    California         1   \n",
      "3  my day so far consisted of work and then nap i...    New Jersey        10   \n",
      "4  the bears want em thats crazy he needs to go t...      New York        11   \n",
      "\n",
      "   AnalyticalResultValue  \n",
      "0               0.322682  \n",
      "1               0.374170  \n",
      "2               0.484683  \n",
      "3               0.339042  \n",
      "4               0.374170  \n",
      "                                           TweetText Closest_State  state_id  \\\n",
      "0  i bet u will think twice b4 u fall to sleep in...      New York        11   \n",
      "1  guess ima go in my room and watch tvgod please...       Georgia         4   \n",
      "2  bout 2 spend sum qt wit mii hunni ive been cra...      New York        11   \n",
      "3  this here is on some truthful shit seems like ...      New York        11   \n",
      "4  val u have like triple gs plz go in ur room an...      Maryland         7   \n",
      "\n",
      "   AnalyticalResultValue  \n",
      "0               0.374170  \n",
      "1               0.322682  \n",
      "2               0.374170  \n",
      "3               0.374170  \n",
      "4               0.359763  \n"
     ]
    }
   ],
   "source": [
    "#Load pfas_data\n",
    "pfas_data_cleaned = pd.read_csv(r\"data\\pfas_data\\merged_state_avg_arv.csv\")\n",
    "\n",
    "# Load processed tweet training data\n",
    "train_tweet_data = pd.read_csv(r\"data\\processed_tweets_train.csv\")\n",
    "\n",
    "# Merge tweet data with PFAS data on 'state_id'\n",
    "train_merged_data = pd.merge(train_tweet_data, pfas_data_cleaned, on='state_id', how='left')\n",
    "\n",
    "# Fill any missing PFAS values with 0 (optional)\n",
    "train_merged_data['AnalyticalResultValue'] = train_merged_data['AnalyticalResultValue'].fillna(0)\n",
    "\n",
    "# Save the merged training dataset\n",
    "train_merged_data.to_csv(r\"data\\train_with_pfas_data.csv\", index=False)\n",
    "print(\"Merged training data saved as 'train_with_pfas_data.csv'.\")\n",
    "\n",
    "# Load processed tweet validation data\n",
    "val_tweet_data = pd.read_csv(r\"data\\processed_tweets_val.csv\")\n",
    "\n",
    "# Merge validation data with PFAS data on 'state_id'\n",
    "val_merged_data = pd.merge(val_tweet_data, pfas_data_cleaned, on='state_id', how='left')\n",
    "\n",
    "# Fill any missing PFAS values with 0 (optional)\n",
    "val_merged_data['AnalyticalResultValue'] = val_merged_data['AnalyticalResultValue'].fillna(0)\n",
    "\n",
    "# Save the merged validation dataset\n",
    "val_merged_data.to_csv(r\"data\\val_with_pfas_data.csv\", index=False)\n",
    "print(\"Merged validation data saved as 'val_with_pfas_data.csv'.\")\n",
    "\n",
    "# Load and inspect the final training data\n",
    "final_train_data = pd.read_csv(r\"data\\train_with_pfas_data.csv\")\n",
    "print(final_train_data.head())\n",
    "\n",
    "# Inspect the validation data\n",
    "final_val_data = pd.read_csv(r\"data\\val_with_pfas_data.csv\")\n",
    "print(final_val_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the merged training and validation datasets\n",
    "train_data = pd.read_csv(r\"data\\train_with_pfas_data.csv\")\n",
    "val_data = pd.read_csv(r\"data\\val_with_pfas_data.csv\")\n",
    "\n",
    "# Debugging\n",
    "# print(\"Columns in training data:\", train_data.columns)\n",
    "# print(\"Columns in validation data:\", val_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_data(data, max_seq_len=50):\n",
    "    # Tokenize the text column\n",
    "    encodings = tokenizer(\n",
    "        data['TweetText'].tolist(),\n",
    "        padding=\"max_length\",    # Pads sequences to the max length\n",
    "        truncation=True,         # Truncates sequences longer than max_seq_len\n",
    "        max_length=max_seq_len,  # Maximum sequence length\n",
    "        return_tensors=\"pt\"      # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # Extract the PFAS values as tensors\n",
    "    pfas_values = torch.tensor(data['AnalyticalResultValue'].values, dtype=torch.float32).unsqueeze(1)  # Shape: [batch_size, 1]\n",
    "    \n",
    "    return encodings, pfas_values\n",
    "\n",
    "# Tokenize the training and validation datasets\n",
    "train_encodings, train_pfas = tokenize_data(train_data, max_seq_len=50)\n",
    "val_encodings, val_pfas = tokenize_data(val_data, max_seq_len=50)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_data['state_id'].values, dtype=torch.long)\n",
    "val_labels = torch.tensor(val_data['state_id'].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Stack tokenized inputs and labels\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    pfas_values = torch.stack([item['pfas'] for item in batch])  # PFAS data\n",
    "\n",
    "    # Preserve original tweet texts as a list\n",
    "    tweet_texts = [item['TweetText'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'pfas': pfas_values,\n",
    "        'TweetText': tweet_texts\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, texts, pfas_values):\n",
    "        self.encodings = encodings  # Pre-tokenized data (input_ids, attention_mask)\n",
    "        self.labels = labels        # Corresponding labels\n",
    "        self.texts = texts          # Original tweet texts\n",
    "        self.pfas_values = pfas_values  # PFAS data as tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve tokenized data\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]  # Add label\n",
    "        item['TweetText'] = self.texts[idx]  # Include original tweet text\n",
    "        item['pfas'] = self.pfas_values[idx]  # Add PFAS data\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_data['TweetText'].tolist()\n",
    "val_texts = val_data['TweetText'].tolist()\n",
    "\n",
    "# PFAS values as tensors\n",
    "train_pfas = torch.tensor(train_data['AnalyticalResultValue'].values, dtype=torch.float32).unsqueeze(1)\n",
    "val_pfas = torch.tensor(val_data['AnalyticalResultValue'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = GeoTextDataset(train_encodings, train_labels, train_texts, train_pfas)\n",
    "val_dataset = GeoTextDataset(val_encodings, val_labels, val_texts, val_pfas)\n",
    "\n",
    "# Create DataLoaders with the updated collate function\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa Model With CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTextModelWithCNN(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, num_states, kernel_sizes=(2, 3, 4), num_filters=64):\n",
    "        super(GeoTextModelWithCNN, self).__init__()\n",
    "        self.text_encoder = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, text_embedding_dim)) for k in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc_cnn = nn.Linear(len(kernel_sizes) * num_filters, 128)\n",
    "        self.fc_pfas = nn.Linear(1, 128)  \n",
    "        self.fc_pfas_predict = nn.Linear(128 + 128, 1) \n",
    "        self.fc_combined = nn.Linear(128 + 128, num_states)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pfas_levels):\n",
    "        # Existing text and CNN processing\n",
    "        text_embedding = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        text_embedding = text_embedding.unsqueeze(1)  # Add channel dimension (for CNN)\n",
    "        \n",
    "        cnn_features = [torch.relu(conv(text_embedding)).squeeze(3) for conv in self.convs]\n",
    "        cnn_features = [torch.max(f, dim=2)[0] for f in cnn_features]\n",
    "        cnn_output = torch.cat(cnn_features, dim=1)\n",
    "        cnn_output = self.fc_cnn(cnn_output)\n",
    "        \n",
    "        # PFAS feature processing\n",
    "        pfas_output = torch.relu(self.fc_pfas(pfas_levels))\n",
    "    \n",
    "        # Combine features\n",
    "        combined_output = torch.cat((cnn_output, pfas_output), dim=1)\n",
    "    \n",
    "        # Predict state\n",
    "        logits_state = self.fc_combined(combined_output)\n",
    "    \n",
    "        # Predict PFAS level\n",
    "        logits_pfas = self.fc_pfas_predict(combined_output)\n",
    "    \n",
    "        # Return both predictions\n",
    "        return logits_state, logits_pfas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> None\n",
      "tweet_texts: 16\n",
      "labels: torch.Size([16])\n",
      "predicted_states: (16,)\n",
      "pfas_levels: torch.Size([16, 1])\n",
      "predicted_pfas: (16,)\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "\n",
    "# outputs = model(input_ids=input_ids, attention_mask=attention_mask, pfas_levels=pfas_levels)\n",
    "# print(type(outputs), outputs.shape if isinstance(outputs, torch.Tensor) else None)\n",
    "\n",
    "# print(\"tweet_texts:\", len(tweet_texts))\n",
    "# print(\"labels:\", labels.shape)\n",
    "# print(\"predicted_states:\", predicted_states.shape)\n",
    "# print(\"pfas_levels:\", pfas_levels.shape)\n",
    "# print(\"predicted_pfas:\", predicted_pfas.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss Implementation\n",
    "        :param alpha: Scaling factor for class imbalance (can be a scalar or a tensor of class weights)\n",
    "        :param gamma: Focusing parameter to down-weight easy examples\n",
    "        :param reduction: 'mean', 'sum', or 'none' for how to reduce the loss\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        :param logits: Model outputs before applying softmax (shape: [batch_size, num_classes])\n",
    "        :param targets: Ground truth class indices (shape: [batch_size])\n",
    "        :return: Computed focal loss\n",
    "        \"\"\"\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Get the probabilities corresponding to the true class\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=logits.size(1))\n",
    "        probs = (probs * targets_one_hot).sum(dim=1)  # Select probabilities for the correct class\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_loss = -self.alpha * (1 - probs) ** self.gamma * torch.log(probs + 1e-9)\n",
    "        \n",
    "        # Reduce loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lemon\\anaconda3\\envs\\test_env_gpu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 44/4654 [00:43<1:15:30,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "# Training and Validation\n",
    "criterion = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "# Early Stopping Parameters\n",
    "early_stop_patience = 2  # Stop if no improvement for these many epochs\n",
    "best_val_accuracy = 0\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "epochs = 10\n",
    "\n",
    "# Path to save the best model\n",
    "best_model_path = r\"model\\geotext_modelcnn.pth\"\n",
    "\n",
    "# Path to save predictions\n",
    "train_predictions_file = r\"data\\predictions\\train_predictions.csv\"\n",
    "val_predictions_file = r\"data\\predictions\\val_predictions.csv\"\n",
    "\n",
    "# Track if headers are written\n",
    "train_header_saved = False\n",
    "val_header_saved = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # ===== TRAINING LOOP =====\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_loader_with_progress = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in train_loader_with_progress:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        pfas_levels = batch[\"pfas\"].to(device)  # Include PFAS levels\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        tweet_texts = batch[\"TweetText\"]  # Access the tweet texts\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits_state, logits_pfas = model(input_ids=input_ids, attention_mask=attention_mask, pfas_levels=pfas_levels)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_state = criterion(logits_state, labels)  # Classification loss for state\n",
    "        loss_pfas = F.mse_loss(logits_pfas.squeeze(), pfas_levels.squeeze())  # Regression loss for PFAS levels\n",
    "        loss = loss_state + loss_pfas  # Combine losses\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Save predictions incrementally\n",
    "        with torch.no_grad():\n",
    "            predicted_states = torch.argmax(logits_state, dim=1).cpu().numpy()\n",
    "            predicted_pfas = logits_pfas.squeeze().cpu().numpy()  # Predicted PFAS levels\n",
    "            batch_results = pd.DataFrame({\n",
    "                'text': tweet_texts,  # Save the TweetText\n",
    "                'true_state': labels.cpu().numpy(),\n",
    "                'predicted_state': predicted_states,\n",
    "                'true_pfas': pfas_levels.squeeze().cpu().numpy(),\n",
    "                'predicted_pfas': predicted_pfas \n",
    "            })\n",
    "\n",
    "            # Write batch predictions to file\n",
    "            if not train_header_saved:\n",
    "                batch_results.to_csv(train_predictions_file, index=False, mode='w')  # Overwrite and write header\n",
    "                train_header_saved = True\n",
    "            else:\n",
    "                batch_results.to_csv(train_predictions_file, index=False, mode='a', header=False)  # Append without header\n",
    "\n",
    "    train_losses.append(total_loss)\n",
    "    print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # ===== VALIDATION LOOP =====\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loader_with_progress = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_with_progress:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            pfas_levels = batch[\"pfas\"].to(device)  # Include PFAS levels\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            tweet_texts = batch[\"TweetText\"]  # Access the tweet texts\n",
    "\n",
    "            logits_state, logits_pfas = model(input_ids=input_ids, attention_mask=attention_mask, pfas_levels=pfas_levels)\n",
    "\n",
    "            # Calculate accuracy for state predictions\n",
    "            predictions = torch.argmax(logits_state, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Save validation predictions incrementally\n",
    "            predicted_states = predictions.cpu().numpy()\n",
    "            predicted_pfas = logits_pfas.squeeze().cpu().numpy()\n",
    "            batch_results = pd.DataFrame({\n",
    "                'text': tweet_texts,\n",
    "                'true_state': labels.cpu().numpy(),\n",
    "                'predicted_state': predicted_states,\n",
    "                'true_pfas': pfas_levels.squeeze().cpu().numpy(),\n",
    "                'predicted_pfas': predicted_pfas\n",
    "            })\n",
    "\n",
    "            # Write batch predictions to file\n",
    "            if not val_header_saved:\n",
    "                batch_results.to_csv(val_predictions_file, index=False, mode='w')  # Overwrite and write header\n",
    "                val_header_saved = True\n",
    "            else:\n",
    "                batch_results.to_csv(val_predictions_file, index=False, mode='a', header=False)  # Append without header\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            torch.save(model.state_dict(), best_model_path)  # Save the best model\n",
    "            print(\"Validation accuracy improved, model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation accuracy. Patience counter: {patience_counter}/{early_stop_patience}\")\n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping triggered. Stopping training after epoch {epoch + 1}.\")\n",
    "            break  # Stop training early\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_file = r\"data\\predictions\\train_predictions.csv\"\n",
    "val_predictions_file = r\"data\\predictions\\val_predictions.csv\"\n",
    "\n",
    "# Load predictions\n",
    "train_preds = pd.read_csv(train_predictions_file)\n",
    "val_preds = pd.read_csv(val_predictions_file)\n",
    "\n",
    "# Compute metrics for validation predictions\n",
    "y_true = val_preds['true_state']\n",
    "y_pred = val_preds['predicted_state']\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Compute Micro and Macro F1 Scores\n",
    "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "# Precision and Recall\n",
    "micro_precision = precision_score(y_true, y_pred, average='micro')\n",
    "macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "micro_recall = recall_score(y_true, y_pred, average='micro')\n",
    "macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f\"Micro Precision: {micro_precision:.4f}, Micro Recall: {micro_recall:.4f}\")\n",
    "print(f\"Macro Precision: {macro_precision:.4f}, Macro Recall: {macro_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, labels=sorted(y_true.unique()))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=sorted(y_true.unique()), yticklabels=sorted(y_true.unique()))\n",
    "plt.xlabel(\"Predicted State\")\n",
    "plt.ylabel(\"True State\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter incorrect predictions for state and PFAS levels\n",
    "error_cases = val_preds[\n",
    "    (val_preds['true_state'] != val_preds['predicted_state']) |  # Mismatched states\n",
    "    (np.abs(val_preds['true_pfas'] - val_preds['predicted_pfas']) > 0.1)  # Significant PFAS level errors\n",
    "]\n",
    "\n",
    "# Display sample error cases\n",
    "print(\"Sample Error Cases:\")\n",
    "print(error_cases[['text', 'true_state', 'predicted_state', 'true_pfas', 'predicted_pfas']].head())\n",
    "error_cases.to_csv(r\"data\\predictions\\val_error_cases.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 scores for each state\n",
    "df_report = df_report[df_report.index.isin(y_true.unique())]  # Filter for relevant states\n",
    "df_report['f1-score'].plot(kind='bar', figsize=(12, 6), title=\"F1 Score per State\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "test_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
