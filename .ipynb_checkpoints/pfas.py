# -*- coding: utf-8 -*-
"""pfas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-yXCu4Zfd2aSf8dIGHm3TfvZBzuLsc0o
"""

#pytorch import for building and training models
import torch

# Check if GPU is available
if torch.cuda.is_available():
    device = torch.device("cuda:0")  # Use the first GPU
    gpu_name = torch.cuda.get_device_name(0)  # Retrieve GPU name

    print("GPU is available")
    print(f"Device: {device}")
    print(f"GPU Name: {gpu_name}")

else:
    print("GPU not available, using CPU")
    device = torch.device("cpu")


# # Mount Google Drive
# from google.colab import drive
# drive.mount('/content/drive')

# !pip install emoji

# !pip install reverse_geocoder

import pandas as pd
import nltk
from nltk import sent_tokenize
import emoji
import re
from html import unescape

from transformers import BertTokenizer, BertModel, AutoTokenizer
import torch

import reverse_geocoder as rg
from tqdm import tqdm

from transformers import AutoTokenizer, DistilBertModel
from torch.utils.data import DataLoader

import nltk
#nltk.download('all')

file_path = "/home/atupulazi/personal_projects/AI Practicum-20241107T183353Z-001/AI-Practicum/Files/GeoText.2010-10-12/full_text.txt"

file_path = file_path
text_file_df = pd.read_csv(file_path, sep="\t", index_col = False, names = ['USER_ID', 'DATE', 'TIME_STAMP', 'LATITUDE', 'LONGITUDE', 'TWEET_TEXT'])

text_file_df.to_csv(file_path + "example_op.csv", index = False)
print (text_file_df)
text_file_df.head()

text_file_df1 = text_file_df.copy()

'''
  Text Cleaning:
    remove userIDs
    ooookay - okay
'''

def clean_data(text_file_df1):
    #each process converts the text value to a string

    # Convert to lowercase
    text_file_df1["TWEET_TEXT"] = text_file_df1["TWEET_TEXT"].apply(lambda x: str(x).lower() if pd.notnull(x) else x)

    # Remove retweet text
    text_file_df1['TWEET_TEXT'] = text_file_df1['TWEET_TEXT'].apply(lambda x: re.sub(r'^rt @\w+:', '', str(x)) if pd.notnull(x) else x)

    # Remove HTML entities like &gt;
    text_file_df1['TWEET_TEXT'] = text_file_df1['TWEET_TEXT'].apply(lambda x: unescape(str(x)) if pd.notnull(x) else x)

    # Remove excessive punctuation (e.g., "!!!" to "!")
    text_file_df1['TWEET_TEXT'] = text_file_df1['TWEET_TEXT'].apply(lambda x: re.sub(r'([!?.])\1+', r'\1', str(x)) if pd.notnull(x) else x)

    # Replace mentions with <USER>
    text_file_df1['TWEET_TEXT'] = text_file_df1['TWEET_TEXT'].apply(lambda x: re.sub(r'@\w+', '<USER>', str(x)) if pd.notnull(x) else x)

    # Remove common emoticons
    text_file_df1['TWEET_TEXT'] = text_file_df1['TWEET_TEXT'].apply(lambda x: re.sub(r"[:;=8xX][-~]?[)DdpP(*/\\]|<3", "", str(x)) if pd.notnull(x) else x)

    # Remove Unicode emojis
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"   # Emoticons
        "\U0001F300-\U0001F5FF"   # Symbols & pictographs
        "\U0001F680-\U0001F6FF"   # Transport & map symbols
        "\U0001F1E0-\U0001F1FF"   # Flags
        "\U00002700-\U000027BF"   # Dingbats
        "\U0001F900-\U0001F9FF"   # Supplemental symbols
        "\U00002600-\U000026FF"   # Misc symbols
        "\U00002B50-\U00002B55"   # Stars and circles
        "]+",
        flags=re.UNICODE
    )
    text_file_df1['TWEET_TEXT'] = text_file_df1['TWEET_TEXT'].apply(lambda x: emoji_pattern.sub("", str(x)) if pd.notnull(x) else x)

    return text_file_df1["TWEET_TEXT"]

text_file_df1['TWEET_TEXT'] = clean_data(text_file_df1)

text_file_df1.to_csv(file_path + "file_op1.csv", index = False)
print(text_file_df1)
text_file_df1.head()

# Convert LATITUDE and LONGITUDE columns to floats
text_file_df1['LATITUDE'] = text_file_df1['LATITUDE'].astype(float)
text_file_df1['LONGITUDE'] = text_file_df1['LONGITUDE'].astype(float)

tqdm.pandas()

# Get all lat/long pairs as a list of tuples
coordinates = list(zip(text_file_df1['LATITUDE'], text_file_df1['LONGITUDE']))

# Batch process the coordinates
locations = rg.search(coordinates)  # This returns a list of dictionaries

# Extract only the 'admin1' field (state) for each location
states = [location['admin1'] for location in locations]

# Add the states as a new column to the DataFrame
text_file_df1['STATE'] = states

#print(text_file_df1['STATE'].value_counts())
print(text_file_df1['STATE'].value_counts().to_string())


text_file_df1.to_csv(file_path + "file_with_states.csv", index = False)

text_file_df2 = text_file_df.copy()

us_states = [
    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',
    'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa',
    'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan',
    'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',
    'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',
    'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',
    'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia',
    'Wisconsin', 'Wyoming'
]

text_file_df2 = text_file_df1[text_file_df1['STATE'].isin(us_states)]
text_file_df2.to_csv(file_path + "file_op_US.csv", index = False)
print (text_file_df2)
print(text_file_df2['STATE'].value_counts().to_string())

# Set the number of tweets to sample per state
n_per_state = 10000  # Adjust this number as needed

# Randomly sample tweets from each state
text_file_df2 = text_file_df2.groupby('STATE', group_keys=False).apply(
    lambda x: x.sample(min(len(x), n_per_state)) #ensures that if a tweet grp is not up to n, it includes the whole group
).reset_index() #keeps the state col bc the new pd ver removes it automatically

print (text_file_df2)
print(text_file_df2['STATE'].value_counts().to_string())



### USING DISTILBERT FOR FASTER PROCESSING ###

#converts texts(treats upper and lower cases the same) to numbers to bert understands
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

#loads the model version to process the embeddings to understand the tweets
model = DistilBertModel.from_pretrained("distilbert-base-uncased").to(device)

batch_size = 64

#converts the tweet column to a list which is compatable with a data structure like dataloader
tweet_texts = text_file_df2['TWEET_TEXT'].tolist()
#load the tweets into the dataloader(tweet loader)
tweet_loader = DataLoader(tweet_texts, batch_size = batch_size)

#storing the embeddings of each tweet
all_embeddings = []

for tweet_batch in tweet_loader:
    tweet_batch = [str(tweet) for tweet in tweet_batch]  # Ensures each item is a string
    inputs = tokenizer(tweet_batch, return_tensors="pt", max_length=64, truncation=True, padding="max_length").to(device)
    with torch.no_grad():
        outputs = model(**inputs)  #Forward pass to get BERT outputs

        # Extract CLS embeddings and move back to CPU to save GPU memory
        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        all_embeddings.extend(cls_embeddings)

# Assign embeddings as a new column in the DataFrame
text_file_df2['CLS_EMBEDDING'] = all_embeddings

text_file_df2.to_csv(file_path + "file_with_embeddings.csv", index = False)

# Apply to each row in the 'TWEET_TEXT' column and save embeddings in a new column

#sample_df = text_file_df1.sample(n=100)  # Process a sample of 100 tweets
#sample_df['CLS_EMBEDDING'] = sample_df['TWEET_TEXT'].apply(get_cls_embedding)

#text_file_df2['CLS_EMBEDDING'] = text_file_df2['TWEET_TEXT'].apply(get_cls_embedding)

