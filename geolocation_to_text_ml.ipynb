{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OB3HZ386Ma21"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aDcfzaBKyDJi"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'data\\full_text\\balanced2_df.csv')\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Closest_State'])\n",
    "\n",
    "# Create LabelEncoder to convert state names to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['State_Label'] = label_encoder.fit_transform(train_df['Closest_State'])\n",
    "val_df['State_Label'] = label_encoder.transform(val_df['Closest_State'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8rnfxENIyDo",
    "outputId": "bbb9b719-f8e9-489c-edc1-726bc92bfda0"
   },
   "outputs": [],
   "source": [
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Data augmentation: Synonym replacement\n",
    "def synonym_replacement(text):\n",
    "    if not isinstance(text, str):  # Check if text is a string\n",
    "        return \"\"  # Return empty string or any default value\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))  # Words that have synonyms\n",
    "\n",
    "    # Your synonym replacement logic goes here...\n",
    "    # For example, replace a random word with a synonym\n",
    "    if random_word_list:\n",
    "        random_word = random.choice(random_word_list)\n",
    "        synonym = random.choice(wordnet.synsets(random_word)).lemmas()[0].name()\n",
    "        new_words = [synonym if word == random_word else word for word in new_words]\n",
    "\n",
    "    return ' '.join(new_words)  # Return modified text\n",
    "\n",
    "# Ensure TweetText has no NaN values\n",
    "train_df = train_df.dropna(subset=['TweetText', 'Closest_City', 'Closest_State'])\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_tweets(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
    "        max_length=100,           # Maximum length of tokens\n",
    "        padding='max_length',     # Pad to max length\n",
    "        truncation=True,          # Truncate longer sequences\n",
    "        return_tensors='pt'       # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Augment training data\n",
    "train_df['Augmented_TweetText'] = train_df['TweetText'].apply(synonym_replacement)\n",
    "train_df['TweetText'] = train_df['TweetText'] + ' ' + train_df['Augmented_TweetText']  # Concatenate original and augmented text\n",
    "\n",
    "# Ensure TweetText has no NaN values in both training and validation sets\n",
    "train_df = train_df.dropna(subset=['TweetText', 'Closest_City', 'Closest_State'])\n",
    "val_df = val_df.dropna(subset=['TweetText', 'Closest_City', 'Closest_State'])  # Drop NaNs in validation set\n",
    "\n",
    "# Generate TF-IDF features for training tweets\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Limit features for simplicity\n",
    "train_tfidf = vectorizer.fit_transform(train_df['TweetText']).toarray()\n",
    "\n",
    "# Generate TF-IDF features for validation tweets\n",
    "val_tfidf = vectorizer.transform(val_df['TweetText']).toarray()  # Transform using the fitted vectorizer\n",
    "\n",
    "# Create input IDs and attention masks for training set\n",
    "train_input_ids = torch.cat([tokenize_tweets(str(text))['input_ids'] for text in train_df['TweetText']])\n",
    "train_attention_masks = torch.cat([tokenize_tweets(str(text))['attention_mask'] for text in train_df['TweetText']])\n",
    "train_labels = train_df['State_Label'].values\n",
    "\n",
    "# Create input IDs and attention masks for validation set\n",
    "val_input_ids = torch.cat([tokenize_tweets(str(text))['input_ids'] for text in val_df['TweetText']])\n",
    "val_attention_masks = torch.cat([tokenize_tweets(str(text))['attention_mask'] for text in val_df['TweetText']])\n",
    "val_labels = val_df['State_Label'].values\n",
    "\n",
    "\n",
    "# Create Hybrid Dataset class\n",
    "class HybridTweetDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, tfidf_features, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.tfidf_features = torch.tensor(tfidf_features, dtype=torch.float32)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'tfidf_features': self.tfidf_features[idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "           \n",
    "# Data loaders\n",
    "train_dataset = HybridTweetDataset(train_input_ids, train_attention_masks, train_tfidf, train_labels)\n",
    "val_dataset = HybridTweetDataset(val_input_ids, val_attention_masks, val_tfidf, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Define hybrid DistilBERT + TF-IDF classifier\n",
    "class HybridDistilBertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, tfidf_size=1000):\n",
    "        super(HybridDistilBertClassifier, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.fc_tfidf = nn.Linear(tfidf_size, 64)  # Dense layer for TF-IDF\n",
    "        self.fc_combined = nn.Linear(self.distilbert.config.hidden_size + 64, num_classes)  # Combine DistilBERT and TF-IDF\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tfidf_features):\n",
    "        distilbert_outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = distilbert_outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        tfidf_output = F.relu(self.fc_tfidf(tfidf_features))  # Pass TF-IDF through dense layer\n",
    "        combined_output = torch.cat((cls_output, tfidf_output), dim=1)  # Concatenate BERT and TF-IDF\n",
    "        logits = self.fc_combined(combined_output)\n",
    "        return logits\n",
    "\n",
    "# Early stopping implementation\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "def train_model(model, train_loader, val_loader, epochs=10, learning_rate=1e-5, start_epoch=0, patience=3, checkpoint_path='models//'):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            tfidf_features = batch['tfidf_features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask, tfidf_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Save model after each epoch\n",
    "        model_save_path = os.path.join(checkpoint_path, f'distilbert_model_epoch_{epoch + 1}.pt')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f'Model saved to {model_save_path}')\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Early stopping check\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            tfidf_features = batch['tfidf_features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, tfidf_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}, F1 Macro: {f1_macro:.4f}, F1 Micro: {f1_micro:.4f}\")\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Load the model from the latest checkpoint if available\n",
    "def load_model(model, checkpoint_path):\n",
    "    # Find all checkpoint files in the directory\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_path, 'distilbert_model_epoch_*.pt'))\n",
    "\n",
    "    if checkpoint_files:\n",
    "        # Sort the files to find the latest one\n",
    "        latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
    "        print(f\"Loading model from {latest_checkpoint}\")\n",
    "\n",
    "        try:\n",
    "            # Load the model state onto the appropriate device\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            model.load_state_dict(torch.load(latest_checkpoint, map_location=device))\n",
    "\n",
    "            # Extract starting epoch from the latest checkpoint filename\n",
    "            start_epoch = int(latest_checkpoint.split('epoch_')[1].split('.pt')[0])\n",
    "\n",
    "            return model, latest_checkpoint, start_epoch\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the model: {e}\")\n",
    "            return model, None, 0\n",
    "    else:\n",
    "        print(\"No checkpoint files found.\")\n",
    "        return model, None, 0\n",
    "\n",
    "# Initialize the model with the number of unique states\n",
    "num_classes = len(df['Closest_State'].unique())  # Update based on your unique states\n",
    "model = HybridDistilBertClassifier(num_classes, tfidf_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "ykjAa5Z7Z8js",
    "outputId": "7021735b-fa26-4435-a12a-b43f21c18968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint files found.\n"
     ]
    }
   ],
   "source": [
    "# Load from checkpoint if available\n",
    "checkpoint_path = 'models//'\n",
    "\n",
    "# Load the model from the latest checkpoint if it exists\n",
    "model, latest_checkpoint, start_epoch = load_model(model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7W29_MQ0kiK",
    "outputId": "2fe98a30-7908-4e99-ee9f-00f82b68a04a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1531/1531 [13:13<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.1187\n",
      "Model saved to models//distilbert_model_epoch_1.pt\n",
      "Validation Accuracy: 0.1243, F1 Macro: 0.0280, F1 Micro: 0.1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1531/1531 [13:09<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 2.9956\n",
      "Model saved to models//distilbert_model_epoch_2.pt\n",
      "Validation Accuracy: 0.1286, F1 Macro: 0.0339, F1 Micro: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1531/1531 [13:08<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 2.7403\n",
      "Model saved to models//distilbert_model_epoch_3.pt\n",
      "Validation Accuracy: 0.1194, F1 Macro: 0.0336, F1 Micro: 0.1194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1531/1531 [13:07<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 2.3677\n",
      "Model saved to models//distilbert_model_epoch_4.pt\n",
      "Validation Accuracy: 0.1075, F1 Macro: 0.0383, F1 Micro: 0.1075\n",
      "Early stopping at epoch 4\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, epochs=10, learning_rate=2e-5, start_epoch=0, patience=3, checkpoint_path='models//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "test_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
